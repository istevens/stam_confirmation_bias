Style guide
===========

- Probability = chance or odds
- Hypothesis, theory = belief

https://en.wikipedia.org/wiki/Truthiness
https://today.yougov.com/news/2016/12/27/belief-conspiracies-largely-depends-political-iden/

Intro
=====

what it is,
why we should care,
how it grows and sticks with us,
why it develops, and
how to fight back.

What it is
==========

.. Need a snappier intro to draw reader in

Confirmation bias is one of many cognitive biases which affect how we reason.
Unlike a leaning or a slant, like left- or right-wing bias, cognitive biases
are the result of involuntary mental "short-cuts". [Brief example] These
short-cuts may have helped our ancestors quickly tell a friend from a foe, but
they impede logic and accuracy necessary in our modern world. [NELSON2015]_

While there are many cognitive biases, confirmation bias likely does us the
most harm. It leads us to hold false beliefs with a confidence greater than
evidence can justify. [NICKERSON1998]_ Those affected will often misinterpret
new information as supporting a previously-held belief. [RABIN1999]_ In this
way, confirmation bias tricks us into accepting untruths and nurtures them
until we are certain they are true.

Before confirmation bias had a name, people were thought to be largely
rational. Any errors in judgment were often blamed on weak
reasoning. [LARRICK2004]_ In his recount of the Peloponnesian War, Thucydides
(460BC-395BC) called confirmation bias a "habit"::

"… for it is a habit of mankind to entrust to careless hope what they long
for, and to use sovereign reason to thrust aside what they do not fancy.

Sir Francis Bacon (1561-1626) was more inclined to treat confirmation bias as a
trick of the mind::

"The human understanding when it has once adopted an opinion (either as
being the received opinion or as being agreeable to itself) draws all
things else to support and agree with it."

It wasn't until 1960, when psychologist Peter Wason performed his first
selection experiment, that confirmation bias was studied and named.

Wason's experiment was simple: present a subject with three numbers (ie.
2-4-6), and ask him/her to identify a rule for the three numbers. The subjects
were then asked to pick three other numbers which fit their supposed rule, and
were told whether their numbers fit the actual rule. Although the actual rule
was "any ascending sequence", subjects would often come up with rules specific
to the initial triplet (eg. "numbers increasing by two") and would often only
pick triplets which confirmed their rule, never triplets which would show it
to be wrong. As we will learn later, improper selection of evidence is one
tendency which contributes to confirmation bias.

Since Wason's experiment, many studies have shown that not only do we hold
systematic biases, they are robust to corrective measures. [LARRICK2004]_ We as
individuals are largely unaware of our own confirmation bias. Worse, although
our reasoning about information is sometimes subject to bias, supporting
existing beliefs, we rationally apply that information to our own state of the
world. Ignorant to our own bias, further reasoning with our beliefs is
tainted. While our bias-influenced reasoning may be internally coherent, it
results in sub-optimal decisions. As we are unaware we are holding a
biased view of the world yet feel we are being rational, we can become
over-confident in our beliefs, and further reinforce our bias. [JONES2000]_
[RABIN1999]_


Why we should care
==================

As we'll learn later, confirmation bias can change the way we view
reality. Once affected, we may only see what we are led to expect. We may also
overvalue information or events which support our theories. Horoscopes are a
mild example of how these tendencies can be used and remain popular because of
them. People often feel that universally positive traits apply to themselves
without considering how widespread those traits actually are. This opens us to
believe and place faith in horoscopes, but also psychics, mind readers, and
other con artists. We often want to believe such people, often focusing on
when they right, not instances when they are wrong. [NICKERSON1998]_

Belief in horoscopes, clairvoyants, and mentalists are mild influences of
confirmation bias. The same tendency to see or remember what we expect or
desire can also feed more serious conditions such as hypochondria and paranoia.
Depressed people may also focus on information which strengthens their
depression, and ignore more positive information which may help them. [NICKERSON1998]_

Of greater concern is how confirmation bias can uphold stereotypes and
prejudice. Our tendency to see what we are led to expect can be detrimental to
what we think of other people or groups. Selective memory of events which
confirm our thinking means that unusual behaviour from distinct groups is more
readily recalled. Several studies have shown how bias can sway how we react
to people about whom we hold stereotypes — even if we are only told those
people belong to a specific group. [NICKERSON1998]_

In one such study, subjects were shown a video of a girl playing. Half the
subjects were told the girl's parents were college-educated who held
white-collar jobs. These subjects were shown the girl playing in a well-to-do
suburban neighbourhood. The other subjects were told the girl's parents were
high-school graduates who held blue-collar jobs, and were shown the girl
playing in a poor inner-city neighbourhood. Half the subjects in each
group were then asked to evaluate the girl's reading level after viewing an
identical video of her answering a series of questions. The group which was
told the girl was from a well-to-do suburban family rated her reading level
significantly higher than the group which was told she was from the inner-city. [RABIN1999]_

This biased tendency to judge based on stereotype doesn't come about after
years of prejudicial thinking either. In a study similar to the one above, 5-7
year-olds were told of a person who was "really, really smart." The children
were then shown a picture of four adults — two women and two men — and asked to
pick the "really, really smart" one. At aged 5, boys and girls chose their
own gender roughly equally. Girls aged 6 or 7, however, were significantly less
likely than boys the same age to view their own gender positively. In another
study with different children, boys and girls aged 6 or 7 were asked to comment
on a game for "really, really smart" children or one for children who "try
really, really hard." The girls were significantly less interested than the
boys in the games for smart children. [BIAN2017]_

Reading these studies, we may feel that are "smarter", that our reasoning is
stronger than others', and that we would not misjudge people, especially a
child, so readily. Yet people well-versed in reasoning and statistics can still
have a problem with confirmation bias. Numerous peer-reviewed studies claim to
show that women are less likely than men to take on risk. However, a 2013
"study of studies" claims that these studies and their authors are likely to be
affected by stereotypes induced by bias. [NELSON2015]_ The studies' authors
reached inaccurate conclusions by falling prey to a number of tendencies behind
confirmation bias.

Some studies on risk and gender reinforce existing gender stereotypes by
inaccurately citing conclusions of earlier literature, or emphasizing results
agreeing with stereotypes, while downplaying or not reporting results which
do not. These confirming results are, in turn, more likely to be published. In
other studies, confounding variables (some due to socialization and pressure to
conform to gender expectations) were neglected. In others, areas where women
naturally take on a great deal of risk (such as with child birth, and risk of
domestic violence) were neglected. Instead, other areas of risk (such as
finance) were studied and findings extrapolated to a broader context.
[NELSON2015]_ In the following paragraphs, we'll learn how tendencies such as
overweighing instances of positive confirmation cause confirmation bias to grow
and persist. Because we often pair these tendencies with internally coherent
patterns of reasoning, few are immune.


Why it develops and persists
============================

Confirmation bias can affect us all, but it doesn't happen by itself. It needs
agreeable conditions to grow, flourish, and persist. Several tendencies can
introduce bias as we develop our belief, while leaving our learning process
intact. All stages of belief development are affected, from our initial
hypothesis generation, to searching for, testing, interpreting, and recalling
evidence. [KLAYMAN1995]_

Sometimes we form a belief from weak evidence, and this is where confirmation
bias can start to take hold. This isn't to say that bias only occurs when
evidence of a belief is not ideal. That first formation of belief, however, is
very powerful, largely due to something called the primacy effect. Information
acquired early carries more weight and is more easily recalled. Belief will
then start to coalesce around those first pieces of information. With belief
backed by initial weak evidence, we may have problems correctly interpreting
better, possibly contradictory information received later. [RABIN1999]_ We
are more likely to question information which conflicts with existing beliefs
than that which agrees with our beliefs. [NICKERSON1998]_ That initial belief,
then, is very important as it is more likely to stick with us and will be
difficult to correct.

Evidence search/selection vs. interpretation
--------------------------------------------

Once we start to form a belief from initial evidence, we will often gather more
data. While we feel that we gather impartial evidence and adjust our belief
accordingly, this is likely not the case. Determining the likelihood that our belief
is true based on other beliefs, each with their own odds of being true, can be
a complex task, and we often fail at it. [#bayes]_ For one, we often prefer positive
tests for belief which can confirm that belief but will not uncover false
negatives. [KLAYMAN1995]_ With Wason's 2-4-6 task as an example, subjects
picked three numbers which fit their theory in order to test it, not
three numbers which would fit a different but also valid theory, or which did not fit
the theory at all.

.. [#bayes] Also known as Bayes' Theorem, this involves calculating the odds
of an event occurring based on conditions related to the event.

[See what one is seeking]

This tendency to seek largely positive evidence to match a theory uncovers
patterns which may not exist, as with Wason's 2-4-6 task, but also limits
discovery. In testing evidence, we tend to ask questions whose answer is "yes" if
the hypothesis is true. For instance, in one study on test selection, participants were given
a profile of an extrovert or an introvert and were asked to interview people to
determine if they fit that type. The questions participants picked were seen as
strongly confirming the personality type under test if given a positive answer,
and strongly disconfirming the type if given a negative answer. [NICKERSON1998]_
This reinforcement of our initial belief through positive tests leads us to be
more confident in our belief, even if the information we collect has no value. [KLAYMAN1995]_ [JONES2000]_

Any selectively collected evidence is then interpreted. Our confirmation bias
kicks in here as well, especially where the evidence is ambiguous or vague. In
instances where evidence is open to interpretation, we tend to give our beliefs
the benefit of the doubt. [KLAYMAN1995]_ As an example, a teacher might
interpret a student's non-standard answer to a question as either stupid or
creative, depending on how the teacher feels about the student beforehand.

We are also prone to view confirming evidence as reliable and relevant, and often
accept it at face value. Disconfirming evidence, by contrast, is often seen as
unreliable and unimportant, and is likely to be scrutinized, often hypercritically,
especially if the source is believed to be subject to error. [RABIN1999]_
[KLAYMAN1995]_ Because of this, we generally require less confirming evidence
to uphold a belief than we do disconfirming evidence to reject one. This
largely depends on our degree of confidence in our belief and the value of
making a correct conclusion. However, our motivation for truth
may be outweighed by our need for self esteem, approval from others, control,
and internal consistency that confirming evidence may provide. [NICKERSON1998]_
In many cases, it may be more imporant for us to maintain our belief preference
than to be accurate. Being wrong can be painful and is often seen as undesirable.
We're also told to "have the courage of one's convictions." [KLAYMAN1995]_

Searching for and interpreting evidence, then, can be an internal fight between
what is right and what feels good. Confirmation bias is not a simple error, but
an internally coherent pattern of reasoning. [JONES2000]_

[Stats failures, modus ponens, contra-positive with Wason's card experiment]


Restricting attention to a favoured belief
------------------------------------------

Seeing what one is seeking (self-fulfilling prophecies, or illusory correlation)
--------------------------------------------------------------------------------


Does learning truly converge on optimizing behaviour?


Why it develops (signals)
=========================


How to fight back
=================

Confirmation bias can occur at every stage of our learning process, from
initial belief to evidence gathering. At every stage, it reinforces itself and
may become so severe that our bias becomes entrenched. Worse, our internal
reasoning remains intact, so we are unaware of our own confirmation bias. Our
battle with bias may seem hopeless, but there are ways in which we can fight or
lessen it.

Although confirmation bias may seem entrenched in our brains, there are
instances where we unknowingly reduce its impact. If we feel we may be punished
for less-than-perfect decisions, our desire for approval can help lessen bias.
"Punishment" could mean a loss of money, a loss of status, or a cost for bad
decisions. Punitive measures are not often available, however. In those
situations, creating an environment which provides a chance to correct and
adjust belief or decisions can also help. [KLAYMAN1995]_

Although a cost for a bad decision can help limit confirmation biases in some
cases, there is little evidence that incentives improve the reliability of our
decision-making. [LARRICK2004]_ [RABIN1999]_ Incentives might work if we feel
that a given task is boring and would otherwise not put in the effort.
Accountability for our decisions, on the other hand, can counter bias in tasks
for which we already possess the appropriate strategy, usually due to
experience in a specific subject. We have a strong social need for consistency,
and are willing to put in the effort and more effectively use information when
making decisions. To avoid embarrassment, we are more likely to foresee flaws
with pre-emptive self-criticism. Our thirst for accountability may go too far,
as we sometimes feel a need to "give people what they want", particularly if we
are undecided. [LARRICK2004]_

Context is also key when making decisions without bias. It helps to have
experience in the area under study, especially if we encounter a problem we
have solved before. Yet confirmation bias often reappears if we try to map
that experience to a different domain. We may also tap into a general schema to
find inconsistencies. Reasoning in areas of duty or obligation — *deontic*
reasoning — such as when a social rule is being broken, can also be relatively
bias-free. [KLAYMAN1995]_

Confirmation bias can sometimes develop if we fail to properly apply formal
reasoning. We may have some basic logic, economics, or statistics knowledge
(such as sampling) but you may not know when or how to use it. If experience
aids to limit confirmation bias, can training help? There is evidence that
short training sessions in a domain with which we're comfortable (such as
sports) can aid us to reduce bias in other areas. That assist, however,
often diminishes over two weeks. [LARRICK2004]_ A more thorough study might be
a better approach, yet little data exists on how specific this training can be
and how generalizable it is.[KLAYMAN1995]_

[Training in biases, rep]

As Nelson's analysis of studies on gender and risk shows, even scholars and
experts are often victims of bias. [NELSON2015]_ There seems to be no guarantee
that intuition can be improved with more education. [KLAYMAN1995]_ Outside
motivation can also only go so far, and may sometimes have the opposite effect.
How then, can we hope to lessen our bias? Formal approaches exist but they are
more geared towards reducing bias in group decisions. We cannot debias
ourselves by ourselves, as we likely don't realise our own biases.
[LARRICK2004]_ As it turns out, the most effective strategy for reducing bias
may be to consider the opposite.

If you've debated a position in school – in English or a debate class, perhaps
– you may have prepared by researching an opposing viewpoint. Considering the
opposite can also be a decent strategy for fighting bias in our beliefs. This
may be as simple as asking ourselves how we may be wrong on a position, why,
and for what reasons. This approach can help reduce overconfidence – a symptom
of confirmation bias – and is shown to lessen bias when looking for and
interpreting new information. [LARRICK2004]_ We reason better with two theories
than when evaluating a single hypothesis. Alternative theories can even come
from other sources. What's important is that we seriously examine a specific
opposing belief. [KLAYMAN1995]_

Naturally, "seriously" examining an alternate belief is key. We may not give an
opposing belief its due, especially if we feel ours is already viable.
[KLAYMAN1995]_ Although directing our attention to contrary evidence can help
counter bias, requiring too many opposing beliefs might backfire. Failing to
come up with a required number of alternate theories might make us more
overconfident in our own. [LARRICK2004]_ Considering more than one theory at
once can also divide our attention. We might prefer to think about alternates
seperately and independently. [KLAYMAN1995]_

.. notes::

   JONES2000::
        - Wason card
            - subjects almost always recognized significance of disconfirmation if found
            - subjects rarely made deductively incorrect judgements
            - learning increases frequency of optimal response, <p, not q> most stable 21/27
                - but no decline in positive confimation response (q card)

   KLAYMAN1995::
        - When does CB go away?
            - possibility of punishment for suboptimal decisions (tie into desire to be right)
            - environment provides opportunity for correction and adjustments
            - depends on strategy paired with environment
                eg. if false positive errors more costly (usually)
                    OF if false negatives more costly
                - positive testing
                - need to adapt and people can if there's a cost

        - knowledge and experience
            - context and content
                - eg. selection task and deontic reasoning
                - abstract vs. rule breaking
                - helps most if problem in area of experience
                    eg. problems solves frequently (CB absent) vs. unfamiliar domains (bias reappears)
                - AND ppl can tap into a general schema to find inconsistencies
                    eg. permission schema and compliance
                - can training help?
                    - yes, but needs to be thorough as brief instructions do not help much
                    - unclear how specific training must be
                        - and how generalizable they can be

        - consider alternatives
            - ppl do better with 2 alternates than evaluating a single hyp
            - mention of specific alts 3x more common if successful subjects than unsuccessful ones
            - consider alts broadens domain and evaluation need not start anew
            - training and real world knowledge can help
                - natural sets of competing hyps known
                - and distinguising feathres get more attention
                - so facilitates comparisons with info => less pseudodiagnostic errors
            - OR use others to gen alts (like journal reviewers)
                - some studies show better hyp development if alts made explicit
                    - OR when asked either/or questions
            - OR discovery
            - difficult to consider >1 hyp at once
                - ppl may think about alts seperately and independently
                - may not seriously consider alts
                    - esp if already have viable hyp

    RABIN1999::
        - more info likely not better
        - Providing same ambiguous info to ppl differing in beliefs can move beliefs further apart
            eg. inner city child and reading
        - to overcome, incentives to collect more info may not pan out
            - so, mute incentives relative to optimal (and no reward for info gathering)
                eg. investment agent offered constant wage

    NELSON2015::
        - bias persists
        - belief as objective => more likely to have confidence in stereotype beliefs and act on them
        - working against stereotype takes more time, uses other areas of brain
            - eg. Francis Bacon quote
        - need wider community of scholars, more diversity of thought and perspective
            - eg. gender, race, class, nationality
            - to reduce locally-held beliefs

    NICKERSON1998::
        - ppl often do not consider p(D|~H)
        - ppl are capable of creating reasons for opposing view if explicitly asked to do so
            - motivational problem, not cognitive limitation
        - ppl more likely to rate one-sided arguments higher than two-sided ones
        - same evidence interpreted differently depending on viewpoint
            - and judged as more consistent than reality

    LARRICK2004::
        (more notes on paper)
        # do better because…motivation
        # replace imperfect strategies with those which approach normative standards
            eg. prescriptive decision-making
                - can approximate normative ideal but can be readily remembered and implemented
                    - meliorists - reasoning falls short but education and experience can improve
                    - apologists - normative standards unavailable, intuitive strategies well-adapted
                - always subset of ppl who give normative response on task
                    - some can do it, so not unattainable
        - technologist: expand strategies to include external techniques (tools)
            - groups not individuals
            - decision aids and info displays
            - formal decision analysis
            - statistical models
            - lone individuals cannot debias selves
                - some biases not easily recognized and corrected
                - will often not realise use of poor decision-making process
                    - delay in feedback on decision
                    - existence and source of error difficult to identify

            - no guarantee that standard econ and stats curricula provide best means for improving intuition

        - little evidence that incentives improve decision-making
            - idea assumes ppl must possess effective strategies and fail to
              apply or apply poorly in absence of incentives
                - effective strategies are complicated (Bayes)
                    OR simple but require correct strategy applied at correct time
            - BUT incentives may work if task is boring leading to lack of effort
                => superficial process

        # accountability for decisions (similar to incentives but with social benefits)
            - embarrassment, impression
            - pre-emptive self-criticism (anticipate flaws)
            - primarily improves performance on tasks for which ppl already possess appropriate strategy
            - leads to greater effort and use fo info => may result in improved performance
            - "lost pilot" if cues unreliable
            - diff with incentives: strong social need for consistency
                - though detrimental, improves prediction when weighing unreliable cues
            - problems
               - "give ppl what they want"
               - if unknown preference, pre-emptive self-criticism

        # consider the opposite
            - how might I be wrong and why? what reasons?
            - effective at reducing overconfidence, hindsight biases, and anchoring effects
            - consider alt hypothesis shown to reduce CB in seeking and evaluating new info
            - also directs attention to contrary evidence
                - BUT requiring too many contrary reasons => can't, so initial hyp correct

        # training in rules (eg. econ, stats)
            - ppl have basic stats, logic, econ knowledge (like sampling)
                BUT may not know how to apply and when
            - short training sessions in comfortable domain (eg. sports)
                - rule generalized to other domains, but diminished over 2 weeks
            - BEST - combine with abstract and concrete examples
                - makes use automatic
            - BUT complex rules like Bayes' a poor candidate - CB

        # training in representation
            - ppl reason more accurately about frequency than probability
                - SO present info as frequencies
                - OR/AND train ppl to translate prop reasoning into frequency reasoning
                - for conditional probability or Bayes'
                    - freq training effective and durable

        # training in biases
            - teach inconsistencies in human reasoning
                - with no instructions to overcome except BEWARE
            - but no controlled experiment with or without recognition skills and decision tools

        # tech strategies
            - out of realm of individual biases
            - group decision-making
                - ppl unknowingly influenced by others judgements => anchor on judgements of others
                - BUT error checking
                    - complementary expertise
                    - increase sample size of experience
                        - beware shared errors and blid spots
                - diversity of experience, training preserve diversity of perspectives
                    - AND formulate own hyp, judgement, estimates independently before group meeting
            - linear models, multi-attribute analysis, decision analysis
                - decompose complex problem into simple problems (eg. pro/con)
            - decision support systems
            - BUT adoption
                - beware top-down, domain-general
                - bottom-up = sense of ownership
                - BUT self-imposed: ppl underestimate bias, are overconfident in their decision-making
                    - fail to recognize needing help
            - approaches encourage ppl to think more deeply otherwise
            - linear models


References
==========

.. [BIAN2017] Bian, L., Leslie, S., and Cimpian, A. (2017). Gender stereotypes
   about intellectual ability emerge early and influence children’s interests.
   Science, 27 Jan 2017, Vol. 355, Issue 6323, pp. 389-391.

.. [JONES2000] Jones, M., and Sugden, R. (2000). Positive confirmation bias in
   the acquisition of information. (Dundee Discussion Papers in Economics; No.
   115). University of Dundee.

.. [KLAYMAN1995] Klayman, J. (1995). Varieties of confirmation bias. In J.
   Busemeyer, R. Hastie, & D. L. Medin (Eds.), Decision making from a cognitive
   perspective. New York: Academic Press (Psychology of Learning and Motivation,
   vol. 32), pp. 365-418.

.. [LARRICK2004] Larrick, R. P. (2004) Debiasing, in Blackwell Handbook of
   Judgment and Decision Making (eds D. J. Koehler and N. Harvey), Blackwell
   Publishing Ltd, Malden, MA, USA.

.. [NELSON2015] Nelson, J. A. (2015), Are women really more risk-averse than
   men? A re-analysis of the literature using expanded methods. Journal of
   Economic Surveys, 29: 566-585.

.. [NICKERSON1998] Nickerson, J. S. (1998). Confirmation bias: a ubiquitous
   phenomenon in many guises. Review of General Psychology, Vol. 2, No. 2, pp.
   175-220.

.. [RABIN1999] Rabin, Matthew and Schrag, Joel L., (1999), First Impressions
   Matter: A Model of Confirmatory Bias, The Quarterly Journal of Economics, 114,
   issue 1, p. 37-82
