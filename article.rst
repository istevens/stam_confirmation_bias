Intro
=====

what it is,
why we should care,
how it grows and sticks with us,
why it develops, and
how to fight back.


What it is
==========

.. Need a snappier intro to draw reader in

Confirmation bias is one of many cognitive biases which affect how we reason.
Unlike a leaning or a slant, like left- or right-wing bias, cognitive biases
are the result of involuntary mental "short-cuts". [Brief example] These
short-cuts may have helped our ancestors quickly tell a friend from a foe, but
they impede logic and accuracy necessary in our modern world. [NELSON2015]_

While there are many cognitive biases, confirmation bias likely does us the
most harm. It leads us to hold false beliefs with a confidence greater than
evidence can justify. [NICKERSON1998]_ Those affected will often misinterpret
new information as supporting a previously-held belief. [RABIN1999]_ In this
way, confirmation bias tricks us into accepting untruths and nurtures them
until we are certain they are true.

Before confirmation bias had a name, people were thought to be largely
rational. Any errors in judgment were often blamed on weak
reasoning. [LARRICK2004]_ In his recount of the Peloponnesian War, Thucydides
(460BC-395BC) called confirmation bias a "habit"::

    "… for it is a habit of mankind to entrust to careless hope what they long
    for, and to use sovereign reason to thrust aside what they do not fancy."

Sir Francis Bacon (1561-1626) was more inclined to treat confirmation bias as a
trick of the mind::

    "The human understanding when it has once adopted an opinion (either as
    being the received opinion or as being agreeable to itself) draws all
    things else to support and agree with it."

It wasn't until 1960, when psychologist Peter Wason performed his first
selection experiment, that confirmation bias was studied and named.

Wason's experiment was simple: present a subject with three numbers (ie.
2-4-6), and ask him/her to identify a rule for the three numbers. The subjects
were then asked to pick three other numbers which fit their supposed rule, and
were told whether their numbers fit the actual rule. Although the actual rule
was "any ascending sequence", subjects would often come up with rules specific
to the initial triplet (eg. "numbers increasing by two") and would often only
pick triplets which confirmed their rule, never triplets which would show it
to be wrong. As we will learn later, improper selection of evidence is one
tendency which contributes to confirmation bias.

Since Wason's experiment, many studies have shown that not only do we hold
systematic biases, they are robust to corrective measures. [LARRICK2004]_ We as
individuals are largely unaware of our own confirmation bias. Worse, although
our reasoning about information is sometimes subject to bias, supporting
existing beliefs, we rationally apply that information to our own state of the
world. Ignorant to our own bias, further reasoning with our beliefs is
tainted. While our bias-influenced reasoning may be internally coherent, it
results in sub-optimal decisions. As we are unaware we are holding a
biased view of the world yet feel we are being rational, we can become
over-confident in our beliefs, and further reinforce our bias. [JONES2000]_
[RABIN1999]_


Why we should care
==================

As we'll learn later, confirmation bias can change the way we view
reality. Once affected, we may only see what we are led to expect. We may also
overvalue information or events which support our theories. Horoscopes are a
mild example of how these tendencies can be used and remain popular because of
them. People often feel that universally positive traits apply to themselves
without considering how widespread those traits actually are. This opens us to
believe and place faith in horoscopes, but also psychics, mind readers, and
other con artists. We often want to believe such people, often focusing on
when they right, not when they are wrong. [NICKERSON1998]_

Belief in horoscopes, clairvoyants, and mentalists are mild influences of
confirmation bias. The same tendency to see or remember what we expect or
desire can also feed more serious conditions such as hypochondria and paranoia.
Depressed people may also focus on information which strengthens their
depression, and ignore more positive information which may help them. [NICKERSON1998]_

Of greater concern is how confirmation bias can uphold stereotypes and
prejudice. Our tendency to see what we are led to expect can be detrimental to
what we think of other people or groups. Selective memory of events which
confirm our thinking means that unusual behaviour from distinct groups is more
readily recalled. Several studies have shown how bias can sway how we react
to people about whom we hold stereotypes — even if we are only told those
people belong to a specific group. [NICKERSON1998]_

In one such study, subjects were shown a video of a girl playing. Half the
subjects were told the girl's parents were college-educated who held
white-collar jobs. These subjects were shown the girl playing in a well-to-do
suburban neighbourhood. The other subjects were told the girl's parents were
high-school graduates who held blue-collar jobs, and were shown the girl
playing in a poor inner-city neighbourhood. Half the subjects in each
group were then asked to evaluate the girl's reading level after viewing an
identical video of her answering a series of questions. The group which was
told the girl was from a well-to-do suburban family rated her reading level
significantly higher than the group which was told she was from the inner-city. [RABIN1999]_

This biased tendency to judge based on stereotype doesn't come about after
years of prejudicial thinking either. In a study similar to the one above, 5-7
year-olds were told of a person who was "really, really smart." The children
were then shown a picture of four adults — two women and two men — and asked to
pick the "really, really smart" one. At aged 5, boys and girls chose their
own gender roughly equally. Girls aged 6 or 7, however, were significantly less
likely than boys the same age to view their own gender positively. In another
study with different children, boys and girls aged 6 or 7 were asked to comment
on a game for "really, really smart" children or one for children who "try
really, really hard." The girls were significantly less interested than the
boys in the games for smart children. [BIAN2017]_

Reading these studies, we may feel that are "smarter", that our reasoning is
stronger than others', and that we would not misjudge people, especially a
child, so readily. Yet people well-versed in reasoning and statistics can still
have a problem with confirmation bias. Numerous peer-reviewed studies claim to
show that women are less likely than men to take on risk. However, a 2013
"study of studies" claims that these studies and their authors are likely to be
affected by stereotypes induced by bias. [NELSON2015]_ The studies' authors
reached inaccurate conclusions by falling prey to a number of tendencies behind
confirmation bias.

Some studies on risk and gender reinforce existing gender stereotypes by
inaccurately citing conclusions of earlier literature, or emphasizing results
agreeing with stereotypes, while downplaying or not reporting results which
do not. These confirming results are, in turn, more likely to be published. In
other studies, confounding variables (some due to socialization and pressure to
conform to gender expectations) were neglected. In others, areas where women
naturally take on a great deal of risk (such as with child birth, and risk of
domestic violence) were neglected. Instead, other areas of risk (such as
finance) were studied and findings extrapolated to a broader context.
[NELSON2015]_ In the following paragraphs, we'll learn how tendencies such as
overweighing instances of positive confirmation can cause confirmation bias to grow
and persist. Because we often pair these tendencies with internally coherent
patterns of reasoning, few are immune.


How it grows and sticks with us
===============================

Confirmation bias can affect us all, but it doesn't happen by itself. It needs
agreeable conditions to grow, flourish, and persist. Several tendencies can
introduce bias as we develop our belief, while leaving our learning process
intact. All stages of belief development are affected, from our initial
hypothesis generation, to searching for, testing, interpreting, and recalling
evidence. [KLAYMAN1995]_

Sometimes we form a belief from weak evidence, and this is where confirmation
bias can start to take hold. This isn't to say that bias only occurs when
evidence of a belief is not ideal. That first formation of belief, however, is
very powerful, largely due to something called the primacy effect. Information
acquired early carries more weight and is more easily recalled. Belief will
then start to coalesce around those first pieces of information. With belief
backed by initial weak evidence, we may have problems correctly interpreting
better, possibly contradictory information received later. [RABIN1999]_ We
are more likely to question information which conflicts with existing beliefs
than that which agrees with our beliefs. [NICKERSON1998]_ That initial belief,
then, is very important as it is more likely to stick with us and will be
difficult to correct.


Evidence search/selection vs. interpretation
--------------------------------------------

Once we start to form a belief from initial evidence, we will often gather more
data. While we feel that we gather impartial evidence and adjust our belief
accordingly, this is likely not the case. Determining the likelihood that our belief
is true based on other beliefs, each with their own odds of being true, can be
a complex task, and we often fail at it. [#bayes]_ For one, we often prefer positive
tests for belief which can confirm that belief but will not uncover false
negatives. [KLAYMAN1995]_ With Wason's 2-4-6 task as an example, subjects
picked three numbers which fit their theory in order to test it, not
three numbers which would fit a different but also valid theory, or which did not fit
the theory at all.

.. [#bayes] Also known as Bayes' Theorem, this involves calculating the odds
   of an event occurring based on conditions related to the event.

[See what one is seeking]

This tendency to seek largely positive evidence to match a theory uncovers
patterns which may not exist, as with Wason's 2-4-6 task, but also limits
discovery. In testing evidence, we tend to ask questions whose answer is "yes" if
the hypothesis is true. For instance, in one study on test selection, participants were given
a profile of an extrovert or an introvert and were asked to interview people to
determine if they fit that type. The questions participants picked were seen as
strongly confirming the personality type under test if given a positive answer,
and strongly disconfirming the type if given a negative answer. [NICKERSON1998]_
This reinforcement of our initial belief through positive tests leads us to be
more confident in our belief, even if the information we collect has no value. [KLAYMAN1995]_ [JONES2000]_

Any selectively collected evidence is then interpreted. Our confirmation bias
kicks in here as well, especially where the evidence is ambiguous or vague.
When evidence is open to interpretation, we tend to give our beliefs
the benefit of the doubt. [KLAYMAN1995]_ As an example, a teacher might
interpret a student's non-standard answer to a question as either stupid or
creative, depending on how the teacher feels about the student beforehand.

We are also prone to view confirming evidence as reliable and relevant, and often
accept it at face value. Disconfirming evidence, by contrast, is often seen as
unreliable and unimportant, and is likely to be scrutinized, often hypercritically,
especially if the source is believed to be subject to error. [RABIN1999]_
[KLAYMAN1995]_ Because of this, we generally require less confirming evidence
to uphold a belief than we do disconfirming evidence to reject one. This
largely depends on our degree of confidence in our belief and the value of
making a correct conclusion. However, our motivation for truth
may be outweighed by our need for self esteem, approval from others, control,
and internal consistency that confirming evidence may provide. [NICKERSON1998]_
In many cases, it may be more important for us to maintain our belief preference
than to be accurate. Being wrong can be painful and is often seen as undesirable.
We're also told to "have the courage of one's convictions." [KLAYMAN1995]_

Searching for and interpreting evidence, then, can be an internal fight between
what is right and what feels good. Confirmation bias is not a simple error, but
an internally coherent pattern of reasoning. [JONES2000]_

[Stats failures, modus ponens, contra-positive with Wason's card experiment]


Restricting attention to a favoured belief
------------------------------------------

Seeing what one is seeking (self-fulfilling prophecies, or illusory correlation)
--------------------------------------------------------------------------------


Does learning truly converge on optimizing behaviour?


Why it develops (signals)
=========================

.. notes::

    - tendency to misinterpret new info as supporting prior belief
        - may lead to erroneously interpreting next signal as supporting

    KLAYMAN1995::
        - self-perception affected more by actions taken, not actions not taken

    RABIN1999::
        - agent may come to believe with near certainty in false hypothesis
          despite receiving and infinite amount of info

        - infinite signals and no confirmatory bias => always correct hypothesis
        - if CB w incorrect hypothesis => bias inhibits ability to overturn erroneous beliefs
            - if strong: agent forever believes very strongly in false hypothesis
            - if severe: learning can exacerbate the bias
        - if convinced, may stop paying attention to additional info
            - cognitive search model
            - process -> strong belief?
                - if yes, stop
                - if no, continue processing

        - belief formation:
            # Start w 50/50 belief hypothesis, no CB
            # Receive a signal of true state of world (assume independent and identically distributed)
                - rational: update belief hyp using Bayes' Rule
                - CB: agent may misinterpret signals which conflict with current belief hyp
                    - alt: agent overlooks evidence conflicting with belief, ignores counterhyp evidence
                    - CB: perceived signals not independently nor identically distributed
                - severity of bias:
                    - if 0, correctly perceives signal
                    - if severe, first piece of info completely determines final belief
                        - so rational observer of agents needs order and prior belief
                - effect depends on signal order and prior belief
                    - overconfident - prob of belief given # signals pro or con compared with prob of alt hyp
                    - underconfident - less than rational, more pro signals
                    - Bayesian observer needs agent's initial belief and order of signals to determine degree of CB
                        eg. agent recently changed hyp => underconfident
                    - info about order of agent's signals would significantly
                      influence observer's judgment and degree of direction of
                      agent's bias

        - after infinite # of signals:
            - fully Bayesian (no bias) => near certainty, correct hyp
            - assume equal # of pro/con signals and 50/50 start
                - agent may become certain that incorrect hyp is true if bias
                  severe OR if bias is slight and signals weak
                    => more and more confident in false belief
                    => take risky and extreme actions (bad choices)
                - OR may be certain of correct hyp
            - learning may not help false belief

    NELSON2015::
        - feels natural => feels incontestable => drawn to confirming

    NICKERSON1998::
        - no stance => take a position => defend/justify position
            - justify: with bias, feedback loop
                - assume hyp is true
        - defend with supported info and selective interpretation (and give it weight)
        - AND do not seek and maybe avoid counter-indicative data/alt. possibilities (and lessen its weight)

        - belief that 2 vars related
            => increase chances of locating confirming evidence
            => decrease chances of locating disconfirming evidence
        - illusory correlation
            - also with higher correlation
            - stereotyping
                - specific behaviours more common with some groups
                - unusual behaviour in distinct groups more readily recalled

    KLAYMAN1995::
        - decision-making is complex
        - too many vars w competing strategies
        - cost of false negative vs false positive
        - likelihood of failure?
        - sense of importance of vars required
        - CB not unitary but property of complex process of hyp development
            => more than one way to fight it


How to fight back
=================

Confirmation bias can occur at every stage of our learning process, from
initial belief to evidence gathering. At every stage, it reinforces itself and
may become so severe that our bias becomes entrenched. Worse, our internal
reasoning remains intact, so we are unaware of our own confirmation bias. Our
battle with bias may seem hopeless, but there are ways in which we can fight or
lessen it.

Although confirmation bias may seem entrenched in our brains, there are
times where we unknowingly reduce its impact. If we feel we may be punished
for less-than-perfect decisions, our desire for approval can help lessen bias.
"Punishment" could mean a loss of money, a loss of status, or a cost for bad
decisions. Punitive measures are not often available, however. In those
situations, creating an environment which provides a chance to correct and
adjust belief or decisions can also help. [KLAYMAN1995]_

Although a cost for a bad decision can help limit confirmation biases in some
cases, there is little evidence that incentives improve the reliability of our
decision-making. [LARRICK2004]_ [RABIN1999]_ Incentives might work if we feel
that a given task is boring and would otherwise not put in the effort.
Accountability for our decisions, on the other hand, can counter bias in tasks
for which we already possess the appropriate strategy, usually due to
experience in a specific subject. We have a strong social need for consistency,
and are willing to put in the effort and more effectively use information when
making decisions. To avoid embarrassment, we are more likely to foresee flaws
with preemptive self-criticism. Our thirst for accountability may go too far,
as we sometimes feel a need to "give people what they want", particularly if we
are undecided. [LARRICK2004]_

Context is also key when making decisions without bias. It helps to have
experience in the area under study, especially if we encounter a problem we
have solved before. Yet confirmation bias often reappears if we try to map
that experience to a different domain. We may also tap into a general schema to
find inconsistencies. Reasoning in areas of duty or obligation — *deontic*
reasoning — such as when a social rule is being broken, can also be relatively
bias-free. [KLAYMAN1995]_

Confirmation bias can sometimes develop if we fail to properly apply formal
reasoning. We may have some basic logic, economics, or statistics knowledge
(such as sampling) but you may not know when or how to use it. If experience
aids to limit confirmation bias, can training help? There is evidence that
short training sessions in a domain with which we're comfortable (such as
sports) can aid us to reduce bias in other areas. That assist, however,
often diminishes over two weeks. [LARRICK2004]_ A more thorough study might be
a better approach, yet little data exists on how specific this training can be
and how generalizable it is. [KLAYMAN1995]_

[Training in biases, rep vs. odds]

As Nelson's analysis of studies on gender and risk shows, even scholars and
experts are often victims of bias. [NELSON2015]_ There seems to be no guarantee
that intuition can be improved with more education. [KLAYMAN1995]_ Outside
motivation can also only go so far, and may sometimes have the opposite effect.
How then, can we hope to lessen our bias? Formal approaches exist but they are
more geared towards reducing bias in group decisions. We cannot debias
ourselves by ourselves, as we likely don't realise our own biases.
[LARRICK2004]_ As it turns out, the most effective strategy for reducing bias
may be to consider the opposite.

If you've debated a position in school – in English or a debate class, perhaps
– you may have prepared by researching an opposing viewpoint. Considering the
opposite can also be a decent strategy for fighting bias in our beliefs. This
may be as simple as asking ourselves how we may be wrong on a position, why,
and for what reasons. This approach can help reduce overconfidence – a symptom
of confirmation bias – and is shown to lessen bias when looking for and
interpreting new information. [LARRICK2004]_ We reason better with two theories
than when evaluating a single hypothesis. Alternative theories can even come
from other sources. What's important is that we seriously examine a specific
opposing belief. [KLAYMAN1995]_

Naturally, *seriously* examining an alternate belief is key. We may not give an
opposing belief its due, especially if we feel ours is already viable.
[KLAYMAN1995]_ Although directing our attention to contrary evidence can help
counter bias, requiring too many opposing viewpoints may backfire. Failing to
come up with a required number of alternate theories might make us more
overconfident in our own. [LARRICK2004]_ Considering more than one theory at
once can also divide our attention. We might prefer to think about alternates
separately and independently. [KLAYMAN1995]_

We may be able to hold our own confirmation bias at bay so long as we are aware
of it, and give serious thought to viewpoints opposed to our own. What about
people that we work with, or our friends?

Unfortunately, when it comes to other individuals, we may just have to grin and
bear it. In the absence of bias, a person could correct their belief with more
information. However, with a person affected by confirmation bias, doing so may
result in the opposite effect, and increase their leanings. Giving the same
ambiguous information to people with differing beliefs may move their beliefs
further apart. [RABIN1999]_ In one study, [cite capital punishment study]
Depending on their viewpoint, others may see the same evidence you do and
interpret differently, judging it as being more consistent with their bias.
[NICKERSON1998]_

Our friends and family with severe bias may be lost to it, but our workplace
can still be saved. Decisions made at work have the advantage in that they
often involve groups, which can be more readily debiased than individuals. Many
strategies for lessening bias in groups exist, usually involving a framework or
a tool to help make sound decisions. Groups can make use of decision aids,
information displays, statistical models, and other formal decision analysis
techniques. Complex problems, say, can be split into smaller, simpler ones and
assigned to smaller groups. These technical strategies are simply out of reach
for most people. Whereas we as individuals can introduce bias at every step of
the decision-making process, groups can track their progress and use those
results as feedback.

Adoption can be a problem when using strategies or tools to make unbiased
decisions at work. A bottom-up approach may have better results than a general
process imposed from the top-down. When the people making the decisions choose
a strategy appropriate to their group, their sense of ownership will help them
stick to it and approach it more honestly. Beware, however, as with ourselves,
groups can also underestimate their own bias and be overconfident in their
decision-making. They, like us, may fail to recognize a need for help. [LARRICK2004]_

Groups are also prone to "group-think". Their members may be influenced by
others, and groups may anchor on the judgments of a few people. Having group
members think about their preferences and estimates before a meeting might help
lessen this risk. Tools and strategies can also check errors in the
decision-making process. It is also a good idea to maintain complementary
expertise within the group, and be aware of blind spots due to shared errors. [LARRICK2004]_

Group-think due to blind spots may be lessened through diversity of experience
within the group. While training can help preserve that diversity of
perspectives, groups can do better by increasing the sample size of experience.
[LARRICK2004]_ Drawing people in from a wider community will increase diversity
of experience and may, in turn, increase diversity of thought. To reduce the
risk of locally-held beliefs, groups should bring in members of differing
genders, ethnicities, social-economic class, and nationality. [NELSON2015]_


References
==========

.. [BIAN2017] Bian, L., Leslie, S., and Cimpian, A. (2017). Gender stereotypes
   about intellectual ability emerge early and influence children’s interests.
   Science, 27 Jan 2017, Vol. 355, Issue 6323, pp. 389-391.

.. [JONES2000] Jones, M., and Sugden, R. (2000). Positive confirmation bias in
   the acquisition of information. (Dundee Discussion Papers in Economics; No.
   115). University of Dundee.

.. [KLAYMAN1995] Klayman, J. (1995). Varieties of confirmation bias. In J.
   Busemeyer, R. Hastie, & D. L. Medin (Eds.), Decision making from a cognitive
   perspective. New York: Academic Press (Psychology of Learning and Motivation,
   vol. 32), pp. 365-418.

.. [LARRICK2004] Larrick, R. P. (2004) Debiasing, in Blackwell Handbook of
   Judgment and Decision Making (eds D. J. Koehler and N. Harvey), Blackwell
   Publishing Ltd, Malden, MA, USA.

.. [NELSON2015] Nelson, J. A. (2015), Are women really more risk-averse than
   men? A re-analysis of the literature using expanded methods. Journal of
   Economic Surveys, 29: 566-585.

.. [NICKERSON1998] Nickerson, J. S. (1998). Confirmation bias: a ubiquitous
   phenomenon in many guises. Review of General Psychology, Vol. 2, No. 2, pp.
   175-220.

.. [RABIN1999] Rabin, Matthew and Schrag, Joel L., (1999), First Impressions
   Matter: A Model of Confirmatory Bias, The Quarterly Journal of Economics, 114,
   issue 1, p. 37-82

https://en.wikipedia.org/wiki/Truthiness

https://today.yougov.com/news/2016/12/27/belief-conspiracies-largely-depends-political-iden/

